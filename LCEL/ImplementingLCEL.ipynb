{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2cf67eb",
   "metadata": {},
   "source": [
    "### LCEL (Langchain expression Language):\n",
    "Practice to create LCELs and Runnables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87fe2154",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "849d87c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import SystemMessage,HumanMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e018b6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatGroq(model=\"llama-3.3-70b-versatile\")\n",
    "outputparser =StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5046b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Static prompt:\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are Smart AI assistant designed to response politely and in details on users query:\"),\n",
    "    HumanMessage(content=\"Steps to implement RAG\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b599fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffd5e158",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RAG (Retrieve, Augment, Generate) is a powerful framework for natural language processing (NLP) and text generation tasks. Implementing RAG involves several steps, which I\\'ll outline below:\\n\\n**Step 1: Define the Task and Objective**\\n Identify the specific NLP task you want to accomplish using RAG, such as text summarization, question answering, or text generation. Clearly define the objective, including the input and output formats, and the desired performance metrics.\\n\\n**Step 2: Choose a Retrieval Method**\\n Select a suitable retrieval method to fetch relevant information from a knowledge base or a large corpus of text. Common retrieval methods include:\\n\\t* Term Frequency-Inverse Document Frequency (TF-IDF)\\n\\t* BM25 (Best Match 25)\\n\\t* Dense Passage Retriever (DPR)\\n\\t* Neural retrievers like BERT-based retrievers\\n\\n**Step 3: Prepare the Knowledge Base**\\n Create or curate a knowledge base that contains relevant information for the task. This can be a large corpus of text, a database, or a knowledge graph. Preprocess the knowledge base by:\\n\\t* Tokenizing the text\\n\\t* Removing stop words and punctuation\\n\\t* Lemmatizing words (optional)\\n\\t* Indexing the text for efficient retrieval\\n\\n**Step 4: Implement the Augment Module**\\n Design and implement an augment module that takes the retrieved information and generates additional context or information to support the task. This can include:\\n\\t* Entity recognition and disambiguation\\n\\t* Coreference resolution\\n\\t* Sentiment analysis or emotion detection\\n\\t* Generating additional text or summaries\\n\\n**Step 5: Implement the Generate Module**\\n Develop a generate module that takes the output from the augment module and generates the final text or response. This can include:\\n\\t* Text generation using a language model (e.g., transformer-based models)\\n\\t* Summarization or text condensation\\n\\t* Question answering or response generation\\n\\n**Step 6: Train and Fine-Tune the Model**\\n Train the RAG model using a suitable dataset and fine-tune it for the specific task. You can use pre-trained language models like BERT, RoBERTa, or XLNet as a starting point and fine-tune them on your dataset.\\n\\n**Step 7: Evaluate and Refine the Model**\\n Evaluate the performance of the RAG model using metrics like accuracy, F1-score, ROUGE score, or BLEU score, depending on the task. Refine the model by:\\n\\t* Adjusting hyperparameters\\n\\t* Experimenting with different retrieval methods or augment modules\\n\\t* Incorporating additional training data or knowledge sources\\n\\n**Step 8: Deploy and Monitor the Model**\\n Deploy the trained RAG model in a production-ready environment and monitor its performance on real-world data. Continuously collect feedback and update the model to maintain its performance and adapt to changing user needs.\\n\\nHere\\'s a high-level example of how you might implement RAG in Python using popular libraries like Hugging Face Transformers and PyTorch:\\n```python\\nimport torch\\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer\\nfrom torch.utils.data import Dataset, DataLoader\\n\\n# Define the dataset and data loader\\nclass RAGDataset(Dataset):\\n    def __init__(self, data, tokenizer):\\n        self.data = data\\n        self.tokenizer = tokenizer\\n\\n    def __getitem__(self, idx):\\n        input_text = self.data[idx][\"input_text\"]\\n        output_text = self.data[idx][\"output_text\"]\\n\\n        inputs = self.tokenizer(input_text, return_tensors=\"pt\")\\n        labels = self.tokenizer(output_text, return_tensors=\"pt\")\\n\\n        return inputs, labels\\n\\n# Load pre-trained language model and tokenizer\\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\\ntokenizer = AutoTokenizer.from_pretrained(\"t5-base\")\\n\\n# Define the RAG model\\nclass RAGModel(torch.nn.Module):\\n    def __init__(self, model, tokenizer):\\n        super(RAGModel, self).__init__()\\n        self.model = model\\n        self.tokenizer = tokenizer\\n\\n    def forward(self, input_text):\\n        # Retrieve relevant information from the knowledge base\\n        retrieved_info = retrieve_info(input_text)\\n\\n        # Augment the retrieved information\\n        augmented_info = augment_info(retrieved_info)\\n\\n        # Generate the final text or response\\n        generated_text = self.model.generate(augmented_info)\\n\\n        return generated_text\\n\\n# Train and evaluate the RAG model\\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\nmodel.to(device)\\n\\ncriterion = torch.nn.CrossEntropyLoss()\\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\\n\\nfor epoch in range(5):\\n    model.train()\\n    for batch in train_dataloader:\\n        input_text, labels = batch\\n        input_text, labels = input_text.to(device), labels.to(device)\\n\\n        optimizer.zero_grad()\\n\\n        outputs = model(input_text)\\n        loss = criterion(outputs, labels)\\n\\n        loss.backward()\\n        optimizer.step()\\n\\n    model.eval()\\n    with torch.no_grad():\\n        total_loss = 0\\n        for batch in val_dataloader:\\n            input_text, labels = batch\\n            input_text, labels = input_text.to(device), labels.to(device)\\n\\n            outputs = model(input_text)\\n            loss = criterion(outputs, labels)\\n            total_loss += loss.item()\\n\\n        print(f\"Epoch {epoch+1}, Val Loss: {total_loss / len(val_dataloader)}\")\\n```\\nNote that this is a simplified example and you may need to modify it to suit your specific use case and requirements. Additionally, implementing RAG can be complex and requires significant expertise in NLP and deep learning.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputparser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82cb108",
   "metadata": {},
   "source": [
    " As the above given code is not good for production ready because we have to update human message manually so we use LCEL for that Rather using simple message prompts we use prompt template that provide context to models as it is Runnable(Object of Langchain that can chain together so that it implement in chain ) we can create chain of it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c143719f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a90304",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unsupported template format: ('user', '{input}')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[43mChatPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_messages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mYou are Smart AI assistant designed to response politely and in details on users query:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{input}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\chat.py:1188\u001b[0m, in \u001b[0;36mChatPromptTemplate.from_messages\u001b[1;34m(cls, messages, template_format)\u001b[0m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_messages\u001b[39m(\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[0;32m   1146\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[0;32m   1147\u001b[0m     template_format: PromptTemplateFormat \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf-string\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1148\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatPromptTemplate:\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[0;32m   1150\u001b[0m \n\u001b[0;32m   1151\u001b[0m \u001b[38;5;124;03m    Examples:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \n\u001b[0;32m   1187\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1188\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\chat.py:961\u001b[0m, in \u001b[0;36mChatPromptTemplate.__init__\u001b[1;34m(self, messages, template_format, **kwargs)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    908\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    912\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    959\u001b[0m \n\u001b[0;32m    960\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 961\u001b[0m     messages_ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    962\u001b[0m         _convert_to_message_template(message, template_format)\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m    964\u001b[0m     ]\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[0;32m    967\u001b[0m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\chat.py:962\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    908\u001b[0m     messages: Sequence[MessageLikeRepresentation],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    912\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    913\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a chat prompt template from a variety of message formats.\u001b[39;00m\n\u001b[0;32m    914\u001b[0m \n\u001b[0;32m    915\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    959\u001b[0m \n\u001b[0;32m    960\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    961\u001b[0m     messages_ \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 962\u001b[0m         \u001b[43m_convert_to_message_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    963\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m messages\n\u001b[0;32m    964\u001b[0m     ]\n\u001b[0;32m    966\u001b[0m     \u001b[38;5;66;03m# Automatically infer input variables from messages\u001b[39;00m\n\u001b[0;32m    967\u001b[0m     input_vars: \u001b[38;5;28mset\u001b[39m[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\chat.py:1457\u001b[0m, in \u001b[0;36m_convert_to_message_template\u001b[1;34m(message, template_format)\u001b[0m\n\u001b[0;32m   1455\u001b[0m     message_ \u001b[38;5;241m=\u001b[39m message\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1457\u001b[0m     message_ \u001b[38;5;241m=\u001b[39m \u001b[43m_create_template_from_message_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1458\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\n\u001b[0;32m   1459\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1460\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mdict\u001b[39m)):\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32md:\\Langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\chat.py:1373\u001b[0m, in \u001b[0;36m_create_template_from_message_type\u001b[1;34m(message_type, template, template_format)\u001b[0m\n\u001b[0;32m   1359\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a message prompt template from a message type and template string.\u001b[39;00m\n\u001b[0;32m   1360\u001b[0m \n\u001b[0;32m   1361\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;124;03m    ValueError: If unexpected message type.\u001b[39;00m\n\u001b[0;32m   1371\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_type \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[1;32m-> 1373\u001b[0m     message: BaseMessagePromptTemplate \u001b[38;5;241m=\u001b[39m \u001b[43mHumanMessagePromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m message_type \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mai\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m   1377\u001b[0m     message \u001b[38;5;241m=\u001b[39m AIMessagePromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\n\u001b[0;32m   1378\u001b[0m         cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m, template), template_format\u001b[38;5;241m=\u001b[39mtemplate_format\n\u001b[0;32m   1379\u001b[0m     )\n",
      "File \u001b[1;32md:\\Langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\chat.py:435\u001b[0m, in \u001b[0;36m_StringImageMessagePromptTemplate.from_template\u001b[1;34m(cls, template, template_format, partial_variables, **kwargs)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a class from a string template.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \n\u001b[0;32m    420\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m    ValueError: If the template is not a string or list of strings.\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(template, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m--> 435\u001b[0m     prompt: Union[StringPromptTemplate, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mPromptTemplate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_template\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemplate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpartial_variables\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartial_variables\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(prompt\u001b[38;5;241m=\u001b[39mprompt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(template, \u001b[38;5;28mlist\u001b[39m):\n",
      "File \u001b[1;32md:\\Langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\prompt.py:304\u001b[0m, in \u001b[0;36mPromptTemplate.from_template\u001b[1;34m(cls, template, template_format, partial_variables, **kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfrom_template\u001b[39m(\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m PromptTemplate:\n\u001b[0;32m    274\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a prompt template from a template.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    *Security warning*:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;124;03m        The prompt template loaded from the template.\u001b[39;00m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m     input_variables \u001b[38;5;241m=\u001b[39m \u001b[43mget_template_variables\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemplate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplate_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m     partial_variables_ \u001b[38;5;241m=\u001b[39m partial_variables \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m partial_variables_:\n",
      "File \u001b[1;32md:\\Langchain\\venv\\lib\\site-packages\\langchain_core\\prompts\\string.py:265\u001b[0m, in \u001b[0;36mget_template_variables\u001b[1;34m(template, template_format)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    264\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnsupported template format: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplate_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(input_variables)\n",
      "\u001b[1;31mValueError\u001b[0m: Unsupported template format: ('user', '{input}')"
     ]
    }
   ],
   "source": [
    "prompts = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are Smart AI assistant designed to response politely and in details on users query:\"),\n",
    "    (\"user\",\"{input}\")\n",
    "    ]\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cfe1e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
